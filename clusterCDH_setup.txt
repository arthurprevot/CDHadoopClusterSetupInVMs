Create box with vagrant, then run following command to finish the setup from within the VMs. 

This is not fully automated for now. Could do that later through chef or puppet which can be called by vagrant.


#------ VM virtualbox setup ------------------------

# Requirements from Cloudera for each VM:
#Make sure we use "Enable IO APIC" but seems ok by default. 
#Change HD controller (in "storage") from SATA to IDE, was required on previous note in cloudera site but not sure if required in this case. Not done and doesn't seem mandatory.


#------ VM setup (on top of vagrant) ------------------------

# Set a password for root for Cloudera manager access on every node (May not be needed though if I know orig password). Needs to be run on every node: 
sudo passwd root

#Also needed by the Cloudera manager for cluster installation:
#sudo apt-get install openssh-client # already setup.
#sudo apt-get install openssh-server # already setup, probably by vagrant

#Make the following changes to /etc/hosts. Not modifying the file will cause a number of cluster startup errors such as not being able to start hBase or creating a number of default directories:

# On each node:
sudo nano /etc/hosts # and put in (this is the same content for every node, each node must know of the address of all other nodes.) :

#----------------------
# Added by AP for cloudera manager, and need to comment both other 127.0.X.1 lines.
#127.0.0.1 precise64 localhost127.0.0.1 localhost.domain localhost222.222.222.201 node1.domain node1222.222.222.202 node2.domain node2222.222.222.203 node3.domain node3
222.222.222.204 node4.domain node4
#----------------------
# Keep the same content on all 4 nodes.
# Had a lot of pb trying different things for this setup so watch out. See https://ccp.cloudera.com/display/CDH4DOC/Configuring+Network+Hosts. Good overal but the checks mentioned there don't work for me, so my setup should not work according to them. Read one part of the solution for me in https://groups.google.com/a/cloudera.org/forum/?fromgroups=#!topic/scm-users/KlJpUlRXcTk. Other one I didn't understand but which solved pb it seems: http://stackoverflow.com/questions/8579105/cloudera-manager-fails-to-add-hosts and in https://groups.google.com/a/cloudera.org/forum/?fromgroups=#!topic/cdh-user/u9NjAg1wQt4.

#Install the GNOME session fallback package:
#sudo apt-get install gnome-session-fallback # didn't do as GUI less anyway for me.

# Install cloudera manager server from node1 (get bin file from cloudera website):
#chmod 755 cloudera-manager-installer.bin # can be 
sudo ./cloudera-manager-installer.bin

Go in cloudera web interface (in web browser on host OS, go to http://222.222.222.201:7180) :
login: admin
pass: admin

# Now will install cloudera manager client on each cluster node.
# On cloudera manager web interface, choose machine to setup on: 222.222.222.[201-204] # should detect ssh server.
# Then cloudera manager will go through installation of CDH on each node (with various components depending on nodes). 
# note: installation on several nodes may fail if all ran in parallel but all from the same machine and from wifi (it worked from LAN connection), so rerun one by one for these that failed and it should work.

# The cluster should now be setup. 